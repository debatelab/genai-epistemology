---
title: "Studying Rational Agency and Epistemic Communities with Large Language Models:<br/>
    Review, How-To, and Reflection"
subtitle: |
    | Workshop on Computational Models in Social Epistemology
    | Bochum, Dec 6-8 2023
    | https://github.com/debatelab/genai-epistemology
author: "Gregor Betz (DebateLab@KIT)"
bibliography: references.bib
format:
  revealjs:
    slide-number: true
    pdf-separate-fragments: true
    theme: [default, custom.scss]
  gfm: default
embed-resources: true
---

# Review

## Autonomous LLM-based in Agents

::: {.r-stack}
![](figs/WangEtAl2023_cover.png){.fragment .fade-in-then-out width=900}

![](figs/WangEtAl2023_trend.png){.fragment .fade-in-then-out}

![](figs/WangEtAl2023_architecture.png){.fragment}
:::

::: {.div style="text-align: right; font-size: 0.7em;"}
ðŸ“„ @wang2023survey
:::


::: {.notes}
Let me first point you to this recent review of LLM-based agents.

More and more highly diverse agents build with LLMs (CoT, ToolFormer)

Blue boxes: Groups of agents that communicate in NL and internally plan and act by generating NL texts

:::


## Artificial Deliberating Agents

::: {.r-stack}
![](figs/Betz2022_cover.png){.fragment .fade-in-then-out width=800}

![](figs/Betz2022_architecture.png){.fragment width=800}
:::

::: {.div style="text-align: right; font-size: 0.7em;"}
ðŸ“„ @betz2022
:::

::: {.notes}

Three years ago, I used LLM-based agents to probe the MÃ¤s-Flache model of argumentative opinion dynamics.

In this model, agents are exchanging reasons that form the basis of their individual beliefs. With LLMs we can build agents that can directly process English sentence (pros and cons in debate), rather than their abstract symbolic representation.

The main findings are:
1. Confirmation: NL models can reproduce basic dynamics of sybolic models
2. Limitations: Allowing agents to generate new reasons (rather than simply exchanging given ones) changes dynamics profoundly
:::

## "Debating" LLMs

::: {.r-stack}
![](figs/DuEtAl2023_cover.png){.fragment .fade-in-then-out width=800}
:::

::: {.div style="text-align: right; font-size: 0.7em;"}
ðŸ“„ @du2023improving
:::

## "Debating" LLMs

**Prompt:** "These are the solutions to the problem from other agents: \<_other agent responses_\> Using the reasoning from other agents as additional advice, can you give an updated answer? Examine your solution and that other agents. Put your answer in the form (X) at the end of your response."

::: {.r-stack}
![](figs/DuEtAl2023_results.png){.fragment fragment-index=1}
:::

::: {.div style="text-align: right;" .fragment fragment-index=1}
ðŸ“„ @du2023improving
:::

# How-To

## Code Boilerplate {.smaller}

::: {.r-stack}
![](figs/ipynb-boilerplate.png){width=800}
:::

[https://github.com/debatelab/genai-epistemology](https://github.com/debatelab/genai-epistemology)

## Skeleton Bounded Confidence Agent

```{.python code-line-numbers="|3-5|7-12|14,17"}
class AbstractBCAgent():

    def update(self, community):
        opinions = [peer.opinion for peer in self.peers(community)]
        self.opinion = self.revise(opinions)

    def peers(self, community):
        peers = [
            agent for agent in community
            if self.distance(agent.opinion) <= epsilon
        ]
        return peers

    def distance(self, opinion):
        pass 

    def revise(self, opinions):
        pass
```

## Numerical BC Agent

```{.python code-line-numbers="|3-5|7-11"}
class NumericalBCAgent(AbstractBCAgent):

    def distance(self, opinion):
        """calculates distance between agent's and other opinion"""
        return abs(opinion - self.opinion)

    def revise(self, opinions):
        """revision through weighted opinion averaging"""
        alpha = self._parameters.get("alpha", .5)
        revision = alpha * self.opinion + (1-alpha) * np.mean(opinions)
        return revision
```

## Numerical BC Model: Results

::: {.r-stack .r-stretch}
![](figs/numbc_opevol.png){.fragment}

![](figs/numbc_diffevol.png){.fragment}
:::


## Numerical BC Agent

```{.python}
class NumericalBCAgent(AbstractBCAgent):

    def distance(self, opinion):
        """calculates distance between agent's and other opinion"""
        return abs(opinion - self.opinion)

    def revise(self, opinions):
        """revision through weighted opinion averaging"""
        alpha = self._parameters.get("alpha", .5)
        revision = alpha * self.opinion + (1-alpha) * np.mean(opinions)
        return revision
```

## Large Language Model

```{.python code-line-numbers="2"}
model = lmql.model(
    "local:HuggingFaceH4/zephyr-7b-alpha",
    device_map = "auto",
    load_in_8bit=True,
    low_cpu_mem_usage=True
)
```

## Agreement Prompt

::: {.r-stack}
![](figs/lmql_agreement.png){width="900"}
:::

## Revision Prompt

::: {.r-stack}
![](figs/lmql_revision.png){width="900"}
:::

## Natural Language BC Agent


```{.python code-line-numbers="|5-7,13-15|8,9"}
class NaturalLanguageBCAgent(AbstractBCAgent):

    def distance(self, other):
        """distance as expected agreement level"""
        lmql_result = agreement_lmq(
            self.opinion, other, **kwargs
        )
        probs = lmql_result.variables.get("P(LABEL)")
        return sum([i*v for i, (_, v) in enumerate(probs)])/4.0

    def revise(self, peer_opinions):
        """natural language opinion revision"""
        revision = revise_lmq(
            self.opinion, peer_opinions, **kwargs
        )
        return revision
```

## Natural Language BC Model: Results

`alpha`="very high"; `epsilon`=[**0.4**]{.fragment .strike fragment-index=2}[**0.5**]{.fragment .fade-in fragment-index=2}; `topic`="veganism"


::: {.r-stack .r-stretch}
![](figs/nlbc_diffevol40.png){.fragment fragment-index=1}

![](figs/nlbc_diffevol50.png){.fragment fragment-index=2}
:::



# Reflection

## Why LLM-based ABMs?

::: incremental
1. Re-create and probe our epistemological (computational) models.
2. Simulate and test scientific methodologies, reasoning modes, principles of rationality.  
  [**Any!**]{.fragment}
  [(E.g. value-free ideal.)]{.fragment}  
  [Without formalizing them.]{.fragment}
:::

::: {.fragment}
ðŸ¤” Are LLMs suited for building epistemic agents?
:::

## ðŸ¤” LLMs' abilities: Reasoning? (1/3)

::: {.r-stack}
![](figs/PanEtAl2023_cover.png){.fragment .fade-in-then-out width=800}

![](figs/PanEtAl2023_overview.png){.fragment width=900}
:::

::: {.div style="text-align: right; font-size: 0.7em;"}
ðŸ“„ @pan2023automatically
:::

## ðŸ¤” LLMs' abilities: Reasoning? (2/3)

::: {.r-stack}
![](figs/MorrisEtAl2023_cover.png){.fragment .fade-in-then-out width=700}

![](figs/MorrisEtAl2023_table.png){.fragment width=550}
:::

::: {.div style="text-align: right; font-size: 0.7em;"}
ðŸ“„ @morris2023levels
:::

## ðŸ¤” LLMs' abilities: Reasoning? (3/3)

::: {.r-stack}
![](figs/MicrosoftAI4Science2023_cover.png){.fragment .fade-in-then-semi-out  width=900}

![](figs/MicrosoftAI4Science2023_highlight.png){.fragment .fade-in-then-out width=950}

![](figs/MicrosoftAI4Science2023_case.png){.fragment width=850}
:::

::: {.div style="text-align: right; font-size: 0.7em;"}
ðŸ“„ @ai4science2023impact
:::

## ðŸ¤” LLMs' abilities: Beliefs?

::: {.r-stack}
![](figs/Betz2023_DoxLM2_cover.png){.fragment .fade-in-then-out width=900}

![](figs/Betz2023_DoxLM2_overview.png){.fragment width=600}
:::

::: {.div style="text-align: right; font-size: 0.7em;"}
ðŸ“„ @BetzRichardson2023
:::

## ðŸ¤” LLMs' abilities: Unhuman?

But humans' cognitive architecture is fundamentally different from LLMs'
[, or is it?]{.fragment}

::: {.r-stack}
![](figs/GoldsteinEtAl2020.png){.fragment width=900}
:::

::: {.div style="text-align: right; font-size: 0.7em;"}
ðŸ“„ @Goldstein2020ThinkingAS
:::

## ðŸ¤” LLMs' abilities: Unhuman? {.smaller}


ðŸ“„ The neural architecture of language: Integrative modeling converges on predictive processing. [@doi:10.1073/pnas.2105646118]

::: {.div style="font-size: 0.8em;"}
TLDR It is found that the most powerful â€œtransformerâ€ models predict nearly 100% of explainable variance in neural responses to sentences and generalize across different datasets and imaging modalities [...].
:::

ðŸ“„ Brains and algorithms partially converge in natural language processing. [@caucheteux2022brains]

::: {.div style="font-size: 0.8em;"}
TLDR This study shows that modern language algorithms partially converge towards brain-like solutions, and thus delineates a promising path to unravel the foundations of natural language processing.
:::

ðŸ“„ Mapping Brains with Language Models: A Survey. [@karamolegkou2023mapping]

::: {.div style="font-size: 0.8em;"}
ABSTRACT [...] We also find that the accumulated evidence, for now, remains ambiguous, but correlations with model size and quality provide grounds for cautious optimism.
:::

ðŸ“„ Artificial neural network language models predict human brain responses to language even after a developmentally realistic amount of training. [@hosseini2022artificial]

::: {.div style="font-size: 0.8em;"}
TLDR [A] developmentally realistic amount of training may suffice and [...] models that have received enough training to achieve sufficiently high next-word prediction performance also acquire representations of sentences that are predictive of human fMRI responses.
:::


## Conclusion

LLMs suited for building epistemic agents?

::: incremental
* âœ… reasoning skill
* âœ… consistent belief states (syn- and diachronic)
* âœ… similarities to human cognition
:::

[Come, join the party! ðŸŽ‰]{.r-fit-text .fragment}


## Conclusion

Vanishing distinctions (due to AGI):

::: incremental
* simulating science *vs* doing science
* epistemology *vs* AI
:::


## Conclusion

Epistemic redundancy (due to AGI) brings profound philosophical challenges:

::: incremental
* What role for humans in science? University?
* Science in a democracy? AGI-proof well-ordered science?
* If AGI is science's ultimate "Freudian" offence and blow to humans' collective narcissism, which revisions of our self-conception (as rational & moral persons) may avoid existentialist disaster? AGI-proof humanism?
:::




# Backup

## References

::: {#refs}
:::